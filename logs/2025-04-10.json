{"asctime": "2025-04-10 22:46:29,573", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 56, in process_query\n    clarification_needed = self.ambiguity_detector.detect_ambiguity(query)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/clarification/ambiguity_detector.py\", line 46, in detect_ambiguity\n    response = self.chain.invoke({\"query\": query})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 331, in invoke\n    self.generate_prompt(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 894, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 719, in generate\n    self._generate_with_cache(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 960, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n    _retry_error_helper(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."}
{"asctime": "2025-04-10 22:46:29,573", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 56, in process_query\n    clarification_needed = self.ambiguity_detector.detect_ambiguity(query)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/clarification/ambiguity_detector.py\", line 46, in detect_ambiguity\n    response = self.chain.invoke({\"query\": query})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 331, in invoke\n    self.generate_prompt(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 894, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 719, in generate\n    self._generate_with_cache(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 960, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n    _retry_error_helper(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."}
{"asctime": "2025-04-10 22:46:43,116", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 56, in process_query\n    clarification_needed = self.ambiguity_detector.detect_ambiguity(query)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/clarification/ambiguity_detector.py\", line 46, in detect_ambiguity\n    response = self.chain.invoke({\"query\": query})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 331, in invoke\n    self.generate_prompt(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 894, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 719, in generate\n    self._generate_with_cache(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 960, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n    _retry_error_helper(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."}
{"asctime": "2025-04-10 22:46:43,116", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 56, in process_query\n    clarification_needed = self.ambiguity_detector.detect_ambiguity(query)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/clarification/ambiguity_detector.py\", line 46, in detect_ambiguity\n    response = self.chain.invoke({\"query\": query})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 331, in invoke\n    self.generate_prompt(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 894, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 719, in generate\n    self._generate_with_cache(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 960, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n    _retry_error_helper(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."}
{"asctime": "2025-04-10 22:46:43,116", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 56, in process_query\n    clarification_needed = self.ambiguity_detector.detect_ambiguity(query)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/clarification/ambiguity_detector.py\", line 46, in detect_ambiguity\n    response = self.chain.invoke({\"query\": query})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 331, in invoke\n    self.generate_prompt(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 894, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 719, in generate\n    self._generate_with_cache(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 960, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n    _retry_error_helper(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."}
{"asctime": "2025-04-10 22:46:43,116", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 56, in process_query\n    clarification_needed = self.ambiguity_detector.detect_ambiguity(query)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/clarification/ambiguity_detector.py\", line 46, in detect_ambiguity\n    response = self.chain.invoke({\"query\": query})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 331, in invoke\n    self.generate_prompt(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 894, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 719, in generate\n    self._generate_with_cache(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 960, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/opt/anaconda3/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n    _retry_error_helper(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-2.0-flashlite is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."}
{"asctime": "2025-04-10 22:52:01,064", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 'QueryProcessor' object has no attribute 'schema'", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 54, in process_query\n    schema_str = self._format_schema_for_prompt()\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 74, in _format_schema_for_prompt\n    for table_name, table_info in self.schema[\"tables\"].items():\n                                  ^^^^^^^^^^^\nAttributeError: 'QueryProcessor' object has no attribute 'schema'"}
{"asctime": "2025-04-10 22:52:01,064", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 'QueryProcessor' object has no attribute 'schema'", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 54, in process_query\n    schema_str = self._format_schema_for_prompt()\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 74, in _format_schema_for_prompt\n    for table_name, table_info in self.schema[\"tables\"].items():\n                                  ^^^^^^^^^^^\nAttributeError: 'QueryProcessor' object has no attribute 'schema'"}
{"asctime": "2025-04-10 22:52:01,064", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 'QueryProcessor' object has no attribute 'schema'", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 54, in process_query\n    schema_str = self._format_schema_for_prompt()\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 74, in _format_schema_for_prompt\n    for table_name, table_info in self.schema[\"tables\"].items():\n                                  ^^^^^^^^^^^\nAttributeError: 'QueryProcessor' object has no attribute 'schema'"}
{"asctime": "2025-04-10 22:52:01,064", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 'QueryProcessor' object has no attribute 'schema'", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 54, in process_query\n    schema_str = self._format_schema_for_prompt()\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 74, in _format_schema_for_prompt\n    for table_name, table_info in self.schema[\"tables\"].items():\n                                  ^^^^^^^^^^^\nAttributeError: 'QueryProcessor' object has no attribute 'schema'"}
{"asctime": "2025-04-10 22:52:01,064", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 'QueryProcessor' object has no attribute 'schema'", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 54, in process_query\n    schema_str = self._format_schema_for_prompt()\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 74, in _format_schema_for_prompt\n    for table_name, table_info in self.schema[\"tables\"].items():\n                                  ^^^^^^^^^^^\nAttributeError: 'QueryProcessor' object has no attribute 'schema'"}
{"asctime": "2025-04-10 22:52:01,064", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: 'QueryProcessor' object has no attribute 'schema'", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 44, in main\n    result = query_processor.process_query(user_query)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 54, in process_query\n    schema_str = self._format_schema_for_prompt()\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/llm_layer/query_processor.py\", line 74, in _format_schema_for_prompt\n    for table_name, table_info in self.schema[\"tables\"].items():\n                                  ^^^^^^^^^^^\nAttributeError: 'QueryProcessor' object has no attribute 'schema'"}
{"asctime": "2025-04-10 22:57:13,475", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 54, in main\n    if result.get('data'):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."}
{"asctime": "2025-04-10 22:57:13,475", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 54, in main\n    if result.get('data'):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."}
{"asctime": "2025-04-10 22:57:30,380", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 54, in main\n    if result.get('data'):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."}
{"asctime": "2025-04-10 22:57:30,380", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 54, in main\n    if result.get('data'):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."}
{"asctime": "2025-04-10 22:57:30,380", "levelname": "ERROR", "name": "hospital_llm", "message": "Error processing query: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().", "exc_info": "Traceback (most recent call last):\n  File \"/Users/diegosanchez/Desktop/Hospital_LLM/Hospital_LLM/src/main.py\", line 54, in main\n    if result.get('data'):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."}
